---
jupyter:
  colab:
  kernelspec:
    display_name: Python 3
    name: python3
  language_info:
    name: python
  nbformat: 4
  nbformat_minor: 0
---

::: {.cell .markdown id="FSQY71GL2Lrq"}
# DIVANIA RAHMADANI

# 121450104 {#121450104}

# TUGAS TBD
:::

::: {.cell .markdown id="lAF4IgNU0zrQ"}
Pada dasarnya, ketika bekerja dengan jumlah gambar yang besar, seperti
dalam kasus penggunaan algoritme jaringan saraf konvolusional (CNN)
untuk tugas-tugas seperti klasifikasi, deteksi, atau segmentasi gambar,
kita perlu mempertimbangkan cara efisien untuk menyimpan dan mengakses
gambar-gambar tersebut.

Salah satu cara yang umum digunakan adalah dengan menyimpan
gambar-gambar tersebut pada disk dalam format file seperti .png atau
.jpg. Pendekatan ini sederhana dan sesuai untuk sebagian besar
kebutuhan, terutama jika kita hanya perlu mengakses gambar secara
individual.

Namun, ketika kita berhadapan dengan jumlah gambar yang sangat besar,
memuat semua gambar tersebut ke dalam memori komputer untuk pelatihan
model menjadi tidak praktis. Inilah saatnya kita mempertimbangkan
penyimpanan yang lebih efisien, seperti menggunakan database.

Salah satu cara untuk menyimpan gambar-gambar dalam jumlah besar adalah
dengan menggunakan database berbasis memori, seperti Lightning
Memory-Mapped Database (LMDB). Dalam LMDB, gambar-gambar disimpan secara
efisien dalam memori terkompresi, memungkinkan akses cepat ke data
bahkan dengan jumlah gambar yang besar.

Selain itu, format data hierarki (HDF5) juga menjadi pilihan yang
populer. HDF5 memungkinkan kita untuk menyimpan data multidimensional,
termasuk gambar, dalam sebuah file tunggal dengan struktur yang
terorganisir. Dengan HDF5, kita dapat menyimpan gambar-gambar beserta
metadata yang terkait dengan cara yang efisien dan mudah diakses.

Dengan menggunakan pendekatan-pendekatan ini, kita dapat mengelola dan
mengakses gambar-gambar dalam jumlah besar dengan lebih efisien,
memungkinkan pelatihan model CNN pada dataset besar seperti ImageNet.
Dengan demikian, meskipun tugas-tugas seperti ini memerlukan waktu yang
cukup lama untuk diproses, kita dapat menjalankannya dengan efisien dan
efektif, memungkinkan kita untuk mengambil langkah-langkah selanjutnya
dalam riset dan pengembangan di bidang visi komputer dan pembelajaran
mesin.
:::

::: {.cell .code execution_count="1" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="3mhl80sRd-Ah" outputId="e5b7c6e9-347f-44cb-9cbf-d9639d68731a"}
``` python
import numpy as np
import pickle
from pathlib import Path

# Path to the unzipped CIFAR data
data_dir = Path("data/cifar-10-batches-py/")

# Unpickle function provided by the CIFAR hosts
def unpickle(file):
    with open(file, "rb") as fo:
        dict = pickle.load(fo, encoding="bytes")
    return dict

images, labels = [], []
for batch in data_dir.glob("data_batch_*"):
    batch_data = unpickle(batch)
    for i, flat_im in enumerate(batch_data[b"data"]):
        im_channels = []
        # Each image is flattened, with channels in order of R, G, B
        for j in range(3):
            im_channels.append(
                flat_im[j * 1024 : (j + 1) * 1024].reshape((32, 32))
            )
        # Reconstruct the original image
        images.append(np.dstack((im_channels)))
        # Save the label
        labels.append(batch_data[b"labels"][i])

print("Loaded CIFAR-10 training set:")
print(f" - np.shape(images)     {np.shape(images)}")
print(f" - np.shape(labels)     {np.shape(labels)}")
```

::: {.output .stream .stdout}
    Loaded CIFAR-10 training set:
     - np.shape(images)     (0,)
     - np.shape(labels)     (0,)
:::
:::

::: {.cell .markdown id="-5U3B-Vi2lq5"}
Perintah di atas digunakan untuk memuat dataset pelatihan CIFAR-10 ke
dalam memori komputer. Dataset CIFAR-10 berisi gambar-gambar kecil
berukuran 32x32 pixel yang terbagi dalam 10 kelas yang berbeda.
:::

::: {.cell .code execution_count="2" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="qJkT8HiLeBxu" outputId="43a68bcc-b42f-4d98-e8b1-31851706f5b0"}
``` python
pip install Pillow
```

::: {.output .stream .stdout}
    Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)
:::
:::

::: {.cell .markdown id="V2YwAO1N3OAJ"}
kode diatas berfungsi untuk menginstal paket Python bernama Pillow.
Pillow adalah fork dari Python Imaging Library (PIL), yang merupakan
pustaka yang digunakan untuk memanipulasi dan memproses gambar dalam
Python. Dengan menginstal Pillow, Anda dapat melakukan berbagai operasi
terkait gambar, seperti membuka, menyimpan, mengubah ukuran,
memanipulasi, dan menerapkan berbagai efek pada gambar
:::

::: {.cell .code execution_count="4" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="4zWz-9eteMjA" outputId="bd4eb907-4dea-462f-e64d-0365e52f3d42"}
``` python
pip install lmdb
```

::: {.output .stream .stdout}
    Collecting lmdb
      Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 299.2/299.2 kB 2.8 MB/s eta 0:00:00
    db
    Successfully installed lmdb-1.4.1
:::
:::

::: {.cell .markdown id="6pInMG0F3Zaa"}
kode diatas digunakan untuk menginstal paket Python bernama LMDB. LMDB
adalah singkatan dari Lightning Memory-Mapped Database, sebuah library
yang menyediakan database berbasis memori yang efisien dan cepat.
Penggunaannya umumnya terlihat dalam aplikasi yang memerlukan
penyimpanan dan pencarian data yang cepat, seperti dalam pengelolaan
besar dataset gambar atau dalam aplikasi kecerdasan buatan yang
memproses data besar secara realtime.
:::

::: {.cell .code execution_count="5" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="1ZaQKam8eVWZ" outputId="63555da4-0174-4f68-8f66-0a06758c1cac"}
``` python
pip install h5py
```

::: {.output .stream .stdout}
    Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)
    Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)
:::
:::

::: {.cell .markdown id="q9asBSaj3vdg"}
Perintah diatas digunakan untuk menginstal paket Python bernama h5py
menggunakan pip, yang merupakan manajer paket standar untuk Python. h5py
adalah pustaka Python yang menyediakan antarmuka untuk menyimpan dan
mengakses data dalam format Hierarchical Data Format version 5 (HDF5).
Ini memungkinkan pengguna untuk bekerja dengan dataset multidimensional
besar dengan struktur data yang kompleks, seperti yang sering digunakan
dalam pengembangan model pembelajaran mesin, analisis data, dan aplikasi
ilmiah lainnya. Dengan menginstal h5py, pengguna dapat memanfaatkan
kekuatan dan fleksibilitas dari format HDF5 dalam proyek
:::

::: {.cell .code execution_count="6" id="Iaq3FA8zeaAA"}
``` python
from pathlib import Path

disk_dir = Path("data/disk/")
lmdb_dir = Path("data/lmdb/")
hdf5_dir = Path("data/hdf5/")
```
:::

::: {.cell .markdown id="FBEABrxC3qFj"}
Perintah di atas digunakan untuk membuat objek Path yang
merepresentasikan direktori di sistem file.
:::

::: {.cell .code execution_count="7" id="B4C1xK6OecaQ"}
``` python
disk_dir.mkdir(parents=True, exist_ok=True)
lmdb_dir.mkdir(parents=True, exist_ok=True)
hdf5_dir.mkdir(parents=True, exist_ok=True)
```
:::

::: {.cell .markdown id="CGywg9H_4LA7"}
Perintah di atas digunakan untuk membuat direktori (folder) pada sistem
file dengan menggunakan modul Path dari library pathlib di Python
:::

::: {.cell .markdown id="MNAIxztPeqo3"}
# **Storing to Disk**
:::

::: {.cell .code execution_count="8" id="Y8lU4jjFeeYZ"}
``` python
from PIL import Image
import csv

def store_single_disk(image, image_id, label):
    """ Stores a single image as a .png file on disk.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    Image.fromarray(image).save(disk_dir / f"{image_id}.png")

    with open(disk_dir / f"{image_id}.csv", "wt") as csvfile:
        writer = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        writer.writerow([label])
```
:::

::: {.cell .markdown id="zgk0qkwf4n0g"}
Perintah di atas digunakan untuk mendefinisikan sebuah fungsi bernama
store_single_disk yang bertujuan untuk menyimpan sebuah gambar tunggal
ke dalam disk dalam format file .png serta menyimpan label dari gambar
tersebut ke dalam sebuah file CSV terpisah.

Fungsi ini menerima tiga parameter:

1.  image: array yang merepresentasikan gambar, dengan dimensi (32, 32,
    3).
2.  image_id: sebuah integer yang merupakan ID unik untuk gambar
    tersebut.
3.  label: label yang menandakan kategori atau kelas dari gambar
    tersebut.

Pertama, gambar disimpan dalam format .png dengan menggunakan fungsi
Image.fromarray(image).save(disk_dir / f\"{image_id}.png\") dari pustaka
PIL (Python Imaging Library). Selanjutnya, label gambar tersebut
disimpan ke dalam sebuah file CSV dengan menggunakan modul csv dari
Python.

Dengan menggunakan fungsi ini, kita dapat dengan mudah menyimpan
gambar-gambar tunggal beserta labelnya ke dalam disk, sehingga
memungkinkan kita untuk melakukan pengolahan dan analisis lebih lanjut
terhadap dataset gambar tersebut.
:::

::: {.cell .markdown id="VtMWfF5-ezuc"}
# **Storing to LMDB**
:::

::: {.cell .code execution_count="9" id="s5-mC6v3ehXh"}
``` python
class CIFAR_Image:
    def __init__(self, image, label):
        # Dimensions of image for reconstruction - not really necessary
        # for this dataset, but some datasets may include images of
        # varying sizes
        self.channels = image.shape[2]
        self.size = image.shape[:2]

        self.image = image.tobytes()
        self.label = label

    def get_image(self):
        """ Returns the image as a numpy array. """
        image = np.frombuffer(self.image, dtype=np.uint8)
        return image.reshape(*self.size, self.channels)
```
:::

::: {.cell .markdown id="JHYfgRos5i0_"}
kode diatas digunakan untuk mewakili sebuah gambar dalam format yang
sesuai untuk dataset CIFAR, yang merupakan dataset yang umum digunakan
untuk pelatihan model dalam pengenalan gambar. Setiap objek CIFAR_Image
menyimpan informasi tentang gambar tersebut, termasuk gambar itu sendiri
dalam bentuk byte dan label yang sesuai.
:::

::: {.cell .code execution_count="10" id="FDGo3UO_e3uO"}
``` python
import lmdb
import pickle

def store_single_lmdb(image, image_id, label):
    """ Stores a single image to a LMDB.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    map_size = image.nbytes * 10

    # Create a new LMDB environment
    env = lmdb.open(str(lmdb_dir / f"single_lmdb"), map_size=map_size)

    # Start a new write transaction
    with env.begin(write=True) as txn:
        # All key-value pairs need to be strings
        value = CIFAR_Image(image, label)
        key = f"{image_id:08}"
        txn.put(key.encode("ascii"), pickle.dumps(value))
    env.close()
```
:::

::: {.cell .markdown id="UFZa9a3T5w2h"}
Perintah di atas digunakan untuk mendefinisikan sebuah fungsi bernama
store_single_lmdb yang bertugas untuk menyimpan sebuah gambar tunggal ke
dalam sebuah database berbasis Lightning Memory-Mapped Database (LMDB).
:::

::: {.cell .markdown id="eN3jScgIe9UE"}
\#**Storing With HDF5**
:::

::: {.cell .code execution_count="11" id="kkC60SGCe50f"}
``` python
import h5py

def store_single_hdf5(image, image_id, label):
    """ Stores a single image to an HDF5 file.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    # Create a new HDF5 file
    file = h5py.File(hdf5_dir / f"{image_id}.h5", "w")

    # Create a dataset in the file
    dataset = file.create_dataset(
        "image", np.shape(image), h5py.h5t.STD_U8BE, data=image
    )
    meta_set = file.create_dataset(
        "meta", np.shape(label), h5py.h5t.STD_U8BE, data=label
    )
    file.close()
```
:::

::: {.cell .markdown id="SP9zc1kz6NFa"}
kode diatas digunakan untuk menyimpan sebuah gambar tunggal ke dalam
sebuah file HDF5. File HDF5 adalah format file yang digunakan untuk
menyimpan data multidimensional dengan struktur yang terorganisir.

1.  Dataset \"image\": Digunakan untuk menyimpan array gambar dalam
    format yang sesuai dengan HDF5.
2.  Dataset \"meta\": Digunakan untuk menyimpan label atau metadata
    lainnya terkait dengan gambar.
:::

::: {.cell .markdown id="NbQ98nAXfGR7"}
\#**Experiments for Storing a Single Image**
:::

::: {.cell .code execution_count="12" id="mq9tpCadfB5V"}
``` python
_store_single_funcs = dict(
    disk=store_single_disk, lmdb=store_single_lmdb, hdf5=store_single_hdf5
)
```
:::

::: {.cell .markdown id="Dnh14HXw6c3H"}
Perintah di atas digunakan untuk membuat kamus yang berisi referensi ke
fungsi-fungsi yang berbeda, yang masing-masing bertanggung jawab untuk
menyimpan satu gambar ke dalam format yang berbeda: disk, LMDB, atau
HDF5. Kamus ini, disebut \'\_store_single_funcs\', memetakan nama metode
penyimpanan (seperti \'disk\', \'lmdb\', atau \'hdf5\') ke fungsi-fungsi
yang sesuai. Dengan cara ini, kita dapat dengan mudah memilih metode
penyimpanan yang ingin kita gunakan dengan hanya merujuk nama metodenya
dari kamus tersebut, tanpa harus menuliskan ulang kode untuk setiap
metode penyimpanan.
:::

::: {.cell .code execution_count="13" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":391}" id="WiRFd_YAfD__" outputId="254ba205-3b5d-4edb-ec24-b7afb1233412"}
``` python
from timeit import timeit

store_single_timings = dict()

for method in ("disk", "lmdb", "hdf5"):
    t = timeit(
        "_store_single_funcs[method](image, 0, label)",
        setup="image=images[0]; label=labels[0]",
        number=1,
        globals=globals(),
    )
    store_single_timings[method] = t
    print(f"Method: {method}, Time usage: {t}")
```

::: {.output .error ename="IndexError" evalue="list index out of range"}
    ---------------------------------------------------------------------------
    IndexError                                Traceback (most recent call last)
    <ipython-input-13-806af442ce80> in <cell line: 5>()
          4 
          5 for method in ("disk", "lmdb", "hdf5"):
    ----> 6     t = timeit(
          7         "_store_single_funcs[method](image, 0, label)",
          8         setup="image=images[0]; label=labels[0]",

    /usr/lib/python3.10/timeit.py in timeit(stmt, setup, timer, number, globals)
        232            number=default_number, globals=None):
        233     """Convenience function to create Timer object and call timeit method."""
    --> 234     return Timer(stmt, setup, timer, globals).timeit(number)
        235 
        236 def repeat(stmt="pass", setup="pass", timer=default_timer,

    /usr/lib/python3.10/timeit.py in timeit(self, number)
        176         gc.disable()
        177         try:
    --> 178             timing = self.inner(it, self.timer)
        179         finally:
        180             if gcold:

    <timeit-src> in inner(_it, _timer)

    IndexError: list index out of range
:::
:::

::: {.cell .markdown id="g9y-0Xw06s7-"}
perintah ini adalah untuk membandingkan kinerja dari berbagai metode
penyimpanan gambar (disk, LMDB, HDF5) dalam hal kecepatan eksekusi untuk
penyimpanan satu gambar.
:::

::: {.cell .code execution_count="15" id="PeMgNi_GfJ7_"}
``` python
import csv
import lmdb
import pickle
from PIL import Image
import h5py
import numpy as np

def store_many_disk(images, labels, disk_dir):
    """ Stores an array of images to disk
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
        disk_dir     directory to save the images and labels
    """
    num_images = len(images)

    # Save all the images one by one
    for i, image in enumerate(images):
        Image.fromarray(image).save(disk_dir / f"{i}.png")

    # Save all the labels to the csv file
    with open(disk_dir / f"{num_images}.csv", "w") as csvfile:
        writer = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for label in labels:
            # This typically would be more than just one value per row
            writer.writerow([label])

def store_many_lmdb(images, labels, lmdb_dir):
    """ Stores an array of images to LMDB.
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
        lmdb_dir     directory to save the LMDB database
    """
    num_images = len(images)

    map_size = num_images * images[0].nbytes * 10

    # Create a new LMDB DB for all the images
    env = lmdb.open(str(lmdb_dir / f"{num_images}_lmdb"), map_size=map_size)

    # Same as before — but let's write all the images in a single transaction
    with env.begin(write=True) as txn:
        for i in range(num_images):
            # All key-value pairs need to be Strings
            value = CIFAR_Image(images[i], labels[i])  # Assuming you have a CIFAR_Image class or similar
            key = f"{i:08}"
            txn.put(key.encode("ascii"), pickle.dumps(value))
    env.close()

def store_many_hdf5(images, labels, hdf5_dir):
    """ Stores an array of images to HDF5.
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
        hdf5_dir     directory to save the HDF5 file
    """
    num_images = len(images)

    # Create a new HDF5 file
    file = h5py.File(hdf5_dir / f"{num_images}_many.h5", "w")

    # Create a dataset in the file
    dataset = file.create_dataset(
        "images", np.shape(images), h5py.h5t.STD_U8BE, data=images
    )
    meta_set = file.create_dataset(
        "meta", np.shape(labels), h5py.h5t.STD_U8BE, data=labels
    )
    file.close()
```
:::

::: {.cell .markdown id="8fh8zNCg64VB"}
Perintah-perintah tersebut digunakan untuk mendefinisikan fungsi-fungsi
yang bertujuan untuk menyimpan array gambar ke dalam berbagai format
penyimpanan:

1.  store_many_disk: Fungsi ini digunakan untuk menyimpan array gambar
    ke dalam format file .png dan menyimpan label-label yang terkait
    dalam format file .csv. Setiap gambar disimpan secara individual
    dengan nama file yang sesuai dengan indeks gambar. Ini adalah
    pendekatan yang sederhana dan sesuai untuk jumlah gambar yang tidak
    terlalu besar.

2.  store_many_lmdb: Fungsi ini digunakan untuk menyimpan array gambar
    ke dalam database LMDB (Lightning Memory-Mapped Database). Dalam
    LMDB, gambar-gambar disimpan secara efisien dalam memori
    terkompresi, memungkinkan akses cepat ke data bahkan dengan jumlah
    gambar yang besar. LMDB cocok digunakan ketika kita perlu mengakses
    gambar-gambar tersebut secara berurutan atau acak.

3.  store_many_hdf5: Fungsi ini digunakan untuk menyimpan array gambar
    ke dalam format file HDF5. HDF5 memungkinkan kita untuk menyimpan
    data multidimensional dalam sebuah file tunggal dengan struktur yang
    terorganisir. Dengan HDF5, kita dapat menyimpan gambar-gambar
    beserta metadata yang terkait dengan cara yang efisien dan mudah
    diakses. HDF5 cocok digunakan ketika kita perlu menyimpan dan
    mengakses banyak gambar dalam satu file dengan struktur yang
    terorganisir.
:::

::: {.cell .markdown id="v98_7dSfgu65"}
# **Preparing the Dataset**
:::

::: {.cell .code execution_count="16" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="Fvz-kX53gYB0" outputId="644e6221-ffca-4a6e-bc95-7c5dd03c5e21"}
``` python
cutoffs = [10, 100, 1000, 10000, 100000]

# Let's double our images so that we have 100,000
images = np.concatenate((images, images), axis=0)
labels = np.concatenate((labels, labels), axis=0)

# Make sure you actually have 100,000 images and labels
print(np.shape(images))
print(np.shape(labels))
```

::: {.output .stream .stdout}
    (0,)
    (0,)
:::
:::

::: {.cell .markdown id="EJymL_sV7SOn"}
Perintah di atas digunakan untuk meningkatkan jumlah gambar dan label
dalam dataset menjadi 100.000. Peningkatan jumlah data ini mungkin
dilakukan untuk menguji performa dan skalabilitas algoritme atau model
pada jumlah data yang lebih besar, yang umumnya memberikan hasil yang
lebih akurat dan dapat dipercaya.
:::

::: {.cell .markdown id="P33acd9mg0UL"}
\#**Experiment for Storing Many Images**
:::

::: {.cell .code id="OJseeAci7t_m"}
``` python
_store_many_funcs = dict(
    disk=lambda images, labels: store_many_disk(images, labels, disk_dir),
    lmdb=lambda images, labels: store_many_lmdb(images, labels, lmdb_dir),
    hdf5=lambda images, labels: store_many_hdf5(images, labels, hdf5_dir)
)

from timeit import timeit

store_many_timings = {"disk": [], "lmdb": [], "hdf5": []}

for cutoff in cutoffs:
    for method in ("disk", "lmdb", "hdf5"):
        t = timeit(
            "_store_many_funcs[method](images_, labels_)",
            setup="images_=images[:cutoff]; labels_=labels[:cutoff]",
            number=1,
            globals=globals(),
        )
        store_many_timings[method].append(t)

        # Print out the method, cutoff, and elapsed time
        print(f"Method: {method}, Time usage: {t}")
```
:::

::: {.cell .markdown id="OUZUbeir7ius"}
Perintah diatas bertujuan untuk melakukan pengukuran waktu ini adalah
untuk membandingkan kinerja dari berbagai metode penyimpanan gambar
(disk, lmdb, hdf5) dan untuk menentukan metode mana yang paling efisien
sesuai dengan kebutuhan aplikasi.
:::

::: {.cell .code id="O-xqUnwCg3cA"}
``` python
import matplotlib.pyplot as plt

def plot_with_legend(
    x_range, y_data, legend_labels, x_label, y_label, title, log=False
):
    """ Displays a single plot with multiple datasets and matching legends.
        Parameters:
        --------------
        x_range         list of lists containing x data
        y_data          list of lists containing y values
        legend_labels   list of string legend labels
        x_label         x axis label
        y_label         y axis label
    """
    plt.style.use("seaborn-whitegrid")
    plt.figure(figsize=(10, 7))

    if len(y_data) != len(legend_labels):
        raise TypeError(
            "Error: number of data sets does not match number of labels."
        )

    all_plots = []
    for data, label in zip(y_data, legend_labels):
        if log:
            temp, = plt.loglog(x_range, data, label=label)
        else:
            temp, = plt.plot(x_range, data, label=label)
        all_plots.append(temp)

    plt.title(title)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend(handles=all_plots)
    plt.show()

# Getting the store timings data to display
disk_x = store_many_timings["disk"]
lmdb_x = store_many_timings["lmdb"]
hdf5_x = store_many_timings["hdf5"]

plot_with_legend(
    cutoffs,
    [disk_x, lmdb_x, hdf5_x],
    ["PNG files", "LMDB", "HDF5"],
    "Number of images",
    "Seconds to store",
    "Storage time",
    log=False,
)

plot_with_legend(
    cutoffs,
    [disk_x, lmdb_x, hdf5_x],
    ["PNG files", "LMDB", "HDF5"],
    "Number of images",
    "Seconds to store",
    "Log storage time",
    log=True,
)
```
:::

::: {.cell .markdown id="5V8rXxHg7_hq"}
Perintah di atas digunakan untuk membuat plot yang menunjukkan waktu
penyimpanan (store timings) dari tiga metode penyimpanan gambar yang
berbeda: PNG files (disk), LMDB, dan HDF5.
:::

::: {.cell .markdown id="_Bwf4oR88EpH"}
# **Reading a Single Image**
:::

::: {.cell .markdown id="3vXNwCE58M2k"}
## **Reading From Disk**
:::

::: {.cell .code id="UOLSdBll8EOT"}
``` python
def read_single_disk(image_id):
    """ Stores a single image to disk.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    image = np.array(Image.open(disk_dir / f"{image_id}.png"))

    with open(disk_dir / f"{image_id}.csv", "r") as csvfile:
        reader = csv.reader(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        label = int(next(reader)[0])

    return image, label
```
:::

::: {.cell .markdown id="hsSl5z518qae"}
kode diatas digunakan untuk membaca satu gambar beserta metadata yang
terkait dari penyimpanan disk. Fungsi ini mengambil parameter image_id,
yang merupakan ID unik untuk gambar yang ingin dibaca.
:::

::: {.cell .markdown id="dENcWHuL9BlD"}
## **Reading From LMDB**
:::

::: {.cell .code id="burTE8D59DVF"}
``` python
def read_single_lmdb(image_id):
    """ Stores a single image to LMDB.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    # Open the LMDB environment
    env = lmdb.open(str(lmdb_dir / f"single_lmdb"), readonly=True)

    # Start a new read transaction
    with env.begin() as txn:
        # Encode the key the same way as we stored it
        data = txn.get(f"{image_id:08}".encode("ascii"))
        # Remember it's a CIFAR_Image object that is loaded
        cifar_image = pickle.loads(data)
        # Retrieve the relevant bits
        image = cifar_image.get_image()
        label = cifar_image.label
    env.close()

    return image, label
```
:::

::: {.cell .markdown id="30fgvjEX9Rs8"}
kode diatas digunakan untuk membaca sebuah gambar dan label yang terkait
dari sebuah database LMDB. Dalam hal ini, gambar-gambar dan label-label
tersebut telah disimpan sebelumnya dalam format tertentu dalam database
LMDB. Fungsi ini mengambil parameter image_id, yang merupakan ID unik
untuk gambar yang ingin dibaca.
:::

::: {.cell .markdown id="1SI_xjp59bBq"}
# **Reading From HDF5**
:::

::: {.cell .code id="JIPf63zM9fxV"}
``` python
def read_single_hdf5(image_id):
    """ Stores a single image to HDF5.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    # Open the HDF5 file
    file = h5py.File(hdf5_dir / f"{image_id}.h5", "r+")

    image = np.array(file["/image"]).astype("uint8")
    label = int(np.array(file["/meta"]).astype("uint8"))

    return image, label
```
:::

::: {.cell .markdown id="OL8Ek9rT-IKt"}
kode diatas berfungsi untuk membuka file HDF5 yang sesuai dengan
image_id, membaca data gambar dari dataset yang disebut \"/image\" dan
metadata dari dataset yang disebut \"/meta\", kemudian mengembalikan
data gambar dan metadata tersebut.
:::

::: {.cell .code id="-YHgaVtj-Ror"}
``` python
_read_single_funcs = dict(
    disk=read_single_disk, lmdb=read_single_lmdb, hdf5=read_single_hdf5
)
```
:::

::: {.cell .markdown id="nqhml1Nu-Y_t"}
Perintah di atas digunakan untuk mendefinisikan sebuah kamus yang
memetakan metode baca tunggal (misalnya membaca satu gambar) ke
fungsi-fungsi yang sesuai untuk membaca gambar dari berbagai sumber
penyimpanan, seperti disk, LMDB, atau HDF5. Kamus ini, disebut
\_read_single_funcs, memungkinkan kita untuk memilih metode baca yang
tepat berdasarkan penyimpanan gambar yang kita gunakan. Setiap kunci
dalam kamus ini adalah string yang menunjukkan metode penyimpanan
(misalnya \'disk\', \'lmdb\', \'hdf5\'), sedangkan nilainya adalah
fungsi-fungsi yang sesuai untuk membaca gambar dari metode penyimpanan
yang diberikan.

Dengan menggunakan pendekatan ini, kita dapat dengan mudah beralih
antara metode-metode pembaca gambar yang berbeda tanpa harus mengubah
kode secara manual di seluruh aplikasi. Ini memungkinkan fleksibilitas
dan modularitas dalam pengelolaan gambar, terutama saat kita perlu
berurusan dengan berbagai sumber data dan penyimpanan yang berbeda.
:::

::: {.cell .markdown id="jWQD3kVZ-jYO"}
# **Experiment for Reading a Single Image**
:::

::: {.cell .code id="ne352YDE-qb2"}
``` python
from timeit import timeit

read_single_timings = dict()

for method in ("disk", "lmdb", "hdf5"):
    t = timeit(
        "_read_single_funcs[method](0)",
        setup="image=images[0]; label=labels[0]",
        number=1,
        globals=globals(),
    )
    read_single_timings[method] = t
    print(f"Method: {method}, Time usage: {t}")
```
:::

::: {.cell .markdown id="lI9DyrIg-t8_"}
Perintah di atas digunakan untuk mengukur waktu yang dibutuhkan untuk
membaca sebuah gambar dari berbagai sumber penyimpanan data, yaitu dari
disk, LMDB (Lightning Memory-Mapped Database), dan HDF5 (Hierarchical
Data Format version 5).
:::

::: {.cell .markdown id="SRowJpYy-zAB"}
\#**Reading Many Images**

\##**Adjusting the Code for Many Images**
:::

::: {.cell .code id="anWORsvf-7iC"}
``` python
def read_many_disk(num_images):
    """ Reads image from disk.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []

    # Loop over all IDs and read each image in one by one
    for image_id in range(num_images):
        images.append(np.array(Image.open(disk_dir / f"{image_id}.png")))

    with open(disk_dir / f"{num_images}.csv", "r") as csvfile:
        reader = csv.reader(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for row in reader:
            labels.append(int(row[0]))
    return images, labels

def read_many_lmdb(num_images):
    """ Reads image from LMDB.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []
    env = lmdb.open(str(lmdb_dir / f"{num_images}_lmdb"), readonly=True)

    # Start a new read transaction
    with env.begin() as txn:
        # Read all images in one single transaction, with one lock
        # We could split this up into multiple transactions if needed
        for image_id in range(num_images):
            data = txn.get(f"{image_id:08}".encode("ascii"))
            # Remember that it's a CIFAR_Image object
            # that is stored as the value
            cifar_image = pickle.loads(data)
            # Retrieve the relevant bits
            images.append(cifar_image.get_image())
            labels.append(cifar_image.label)
    env.close()
    return images, labels

def read_many_hdf5(num_images):
    """ Reads image from HDF5.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []

    # Open the HDF5 file
    file = h5py.File(hdf5_dir / f"{num_images}_many.h5", "r+")

    images = np.array(file["/images"]).astype("uint8")
    labels = np.array(file["/meta"]).astype("uint8")

    return images, labels

_read_many_funcs = dict(
    disk=read_many_disk, lmdb=read_many_lmdb, hdf5=read_many_hdf5
)
```
:::

::: {.cell .markdown id="nMpYKKFq_CY2"}
Perintah di atas digunakan untuk mendefinisikan fungsi-fungsi yang
digunakan untuk membaca gambar dari tiga jenis penyimpanan yang berbeda:
disk, LMDB, dan HDF5.
:::

::: {.cell .markdown id="ZFxeyu4U_L60"}
\#**Experiment for Reading Many Images**
:::

::: {.cell .code id="U4UY0JWy_XjL"}
``` python
from timeit import timeit

read_many_timings = {"disk": [], "lmdb": [], "hdf5": []}

for cutoff in cutoffs:
    for method in ("disk", "lmdb", "hdf5"):
        t = timeit(
            "_read_many_funcs[method](num_images)",
            setup="num_images=cutoff",
            number=1,
            globals=globals(),
        )
        read_many_timings[method].append(t)

        # Print out the method, cutoff, and elapsed time
        print(f"Method: {method}, No. images: {cutoff}, Time usage: {t}")
```
:::

::: {.cell .markdown id="BhTB4a_Z_qoz"}
Perintah di atas digunakan untuk mengukur waktu yang diperlukan untuk
membaca sejumlah besar gambar dari tiga metode penyimpanan yang berbeda:
disk, LMDB (Lightning Memory-Mapped Database), dan HDF5 (Hierarchical
Data Format version 5).

Pertama, itu melakukan iterasi melalui jumlah gambar yang berbeda
(disimpan dalam variabel cutoffs). Kemudian, untuk setiap jumlah gambar,
itu mengukur waktu yang diperlukan untuk membaca gambar menggunakan
masing-masing metode penyimpanan (disk, LMDB, dan HDF5).

Fungsi timeit digunakan untuk mengukur waktu eksekusi dari
\_read_many_funcs \[method\] (num_images) (di mana method adalah metode
penyimpanan yang sedang diukur dan num_images adalah jumlah gambar yang
akan dibaca). Setup digunakan untuk menyiapkan kondisi yang diperlukan
sebelum mengukur waktu (misalnya, mengatur jumlah gambar yang akan
dibaca).

Hasil waktu yang diukur kemudian dicatat dalam struktur data
read_many_timings, yang mengelompokkan waktu pembacaan berdasarkan
metode penyimpanan. Setelah itu, informasi tentang metode, jumlah
gambar, dan waktu yang dihabiskan dicetak untuk memungkinkan analisis
kinerja dari berbagai metode penyimpanan.
:::

::: {.cell .markdown id="Z3S2diU9_gkF"}
\#**Considering Disk Usage**

\##**A More Critical Look at Implementation**
:::

::: {.cell .code id="b-BbmBN3_iGe"}
``` python
# Slightly slower
for i in range(len(dataset)):
    # Read the ith value in the dataset, one at a time
    do_something_with(dataset[i])

# This is better
data = dataset[:]
for d in data:
    do_something_with(d)
```
:::

::: {.cell .markdown id="cBHE2EGJAPzO"}
Perintah di atas digunakan untuk mengiterasi melalui dataset menggunakan
dua pendekatan yang berbeda.

1.  Pendekatan pertama menggunakan loop for untuk mengakses setiap
    elemen dataset secara berurutan menggunakan indeks. Dalam setiap
    iterasi, nilai ke-i dari dataset diambil menggunakan dataset\[i\]
    dan dioperasikan dengan do_something_with(). Meskipun pendekatan ini
    sederhana, itu bisa sedikit lambat karena ada overhead dalam
    mengakses setiap elemen secara terpisah.

2.  Pendekatan kedua menggunakan slicing (dataset\[:\]) untuk menyalin
    seluruh dataset ke dalam variabel data. Kemudian, loop for digunakan
    untuk mengiterasi melalui data. Pendekatan ini lebih cepat karena
    hanya ada satu operasi pembacaan untuk seluruh dataset, dan setelah
    itu, iterasi dilakukan secara lokal pada variabel data, yang
    merupakan salinan dataset. Ini mengurangi jumlah operasi pembacaan
    dan meningkatkan efisiensi.

Pendekatan kedua umumnya lebih disukai karena lebih cepat dan memiliki
overhead yang lebih rendah daripada pendekatan pertama. Ini adalah
contoh yang bagus dari bagaimana memperhatikan kinerja kode Anda dapat
menghasilkan perbedaan yang signifikan dalam kasus-kasus di mana Anda
berurusan dengan jumlah data yang besar.
:::
